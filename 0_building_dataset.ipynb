{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import string\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New dataset\n",
    "1. Take list of all 3-letter acronyms that are tokenized as three separate tokens (e.g. \"|A|B|C|\")\n",
    "2. Take list of words that are tokenized as a single token (including their preceding space, e.g. \"| Bruh|\")\n",
    "3. Create list of samples by (i) choosing and acronym and (ii) sampling a word from the previous list for every capital letter of the acronym (e.g. \"|The| Auto| Bruh| Cafe|(|A|B|C|\"). **NOTE:** We will sample the acronyms according to the frequency of the capital letters on our dictionary. We do this because, for example, there are a lot of acronyms containing the letter X but we have just one word beginning by X on our dictionary (as these are uncommon) thus uniformly sampling the acronyms would bring the word X a lot more than usual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acronyms_list = list(product(string.ascii_uppercase, repeat=3))\n",
    "acronyms_list = [\"\".join(x) for x in acronyms_list]\n",
    "len(acronyms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms_tokenized = model.to_tokens(acronyms_list, prepend_bos=False)\n",
    "device = acronyms_tokenized.device\n",
    "indices = torch.arange(acronyms_tokenized.shape[0], device=device)[acronyms_tokenized[:, -1] != 50256]\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_acronyms_list = model.to_string(acronyms_tokenized[indices])\n",
    "len(possible_acronyms_list), possible_acronyms_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2740 3-letter acronyms that are tokenized as three separate words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's collect a set of nouns that are tokenized as single tokens.\n",
    "\n",
    "We will take the words from https://github.com/taikuukaits/SimpleWordlists/tree/master (Ashley Bovan page). It contains around **91k** nouns, so it should be sufficient (some of them are quite rare):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/nouns_1_token.txt\", \"w\") as g:\n",
    "    with open(\"data/Wordlist-Nouns-All.txt\", \"r\") as f:\n",
    "        for word in f.read().splitlines():\n",
    "            word = \" \" + word.capitalize()\n",
    "            str_word = model.to_str_tokens(word, prepend_bos=False)\n",
    "            if len(str_word) == 1:\n",
    "                g.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/nouns_1_token.txt\", \"r\") as f:\n",
    "    print(len(f.read().splitlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When filtering the nouns that are not tokenized as single tokens (including a preceding space) we drop from 91k to 7k. Now, to make it easier, we will build a dictionary that groups words beginning with the same capital letter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary\n",
    "words_dict = {}\n",
    "for c in string.ascii_uppercase:\n",
    "    words_dict[c] = []\n",
    "\n",
    "n_words = 0\n",
    "with open(\"data/nouns_1_token.txt\", \"r\") as f:\n",
    "    for word in f.read().splitlines():\n",
    "        words_dict[word[1]].append(word)\n",
    "        n_words += 1\n",
    "\n",
    "with open('data/words_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(words_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/words_dict.pkl\", 'rb') as f:\n",
    "        words_dict = pickle.load(f)\n",
    "\n",
    "prob_dict = {}\n",
    "\n",
    "n_words = sum([len(v) for _, v in words_dict.items()])\n",
    "\n",
    "# get the probability of sampling a word beginning by each letter\n",
    "for k, v in words_dict.items():\n",
    "    prob_dict[k] = float(len(v)) / n_words\n",
    "\n",
    "plt.bar(x=prob_dict.keys(), height=prob_dict.values(), edgecolor=\"black\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability distribution of the acronym list\n",
    "p = np.array([prob_dict[acronym[0]]*prob_dict[acronym[1]]*prob_dict[acronym[2]] for acronym in possible_acronyms_list])\n",
    "# normalize\n",
    "p = p / p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_acronym(acronyms_list=possible_acronyms_list, words_dict_path=\"data/words_dict.pkl\"):\n",
    "    \n",
    "    with open(words_dict_path, 'rb') as f:\n",
    "        words_dict = pickle.load(f)\n",
    "\n",
    "    acronym = np.random.choice(acronyms_list, size=1, p=p)[0]\n",
    "    prompt = \"The\"\n",
    "    for c in acronym:\n",
    "        # Obtain a random word that meets the requirements\n",
    "        word = random.choice(words_dict[c])\n",
    "        prompt += word\n",
    "    prompt = prompt + \" (\" + acronym[:2]\n",
    "    return prompt, acronym\n",
    "\n",
    "prompt, acronym = get_random_acronym()\n",
    "prompt, model.to_string(model(prompt)[:, -1].argmax(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the sampling method, we will retrieve a list of 100000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "with open(\"data/acronyms.txt\", \"w\") as f:\n",
    "    while i < 10000:    \n",
    "        prompt, acronym = get_random_acronym()\n",
    "        f.write(prompt + \", \" + acronym + \"\\n\")\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
